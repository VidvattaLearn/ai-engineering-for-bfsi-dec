{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf031b89",
   "metadata": {},
   "source": [
    "# DeepEval for RAG and Agent Evaluations\n",
    "\n",
    "This notebook demonstrates comprehensive evaluation strategies using DeepEval for:\n",
    "\n",
    "## üéØ Evaluation Types\n",
    "\n",
    "### 1. RAG System Evaluations\n",
    "- **Answer Relevancy**: Measures if answers are relevant to questions\n",
    "- **Faithfulness**: Detects hallucinations by checking grounding in context\n",
    "- **Contextual Relevancy**: Validates retrieved context quality\n",
    "- **Correctness (G-Eval)**: Compares actual vs expected outputs\n",
    "\n",
    "### 2. AI Agent Evaluations\n",
    "- **Tool Correctness**: Validates correct tool selection\n",
    "- **Tool Arguments**: Verifies proper tool invocation\n",
    "- **Expected Output Comparison**: Compares agent outputs with expected results\n",
    "\n",
    "## üìä Data Sources\n",
    "- **Vector Store**: Pinecone with real insurance policy documents\n",
    "- **Database**: SQLite with sales data for agent testing\n",
    "- **LLM**: Azure OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8907dc",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages for DeepEval, LangChain, and vector store integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51387b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq deepeval langchain langchain-openai langgraph langchain-pinecone langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199fa1d0",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "Configure Azure OpenAI, LangSmith tracing, and Pinecone credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca949ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-12-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-agents-sept-cohort-resource.cognitiveservices.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# LangSmith Tracing\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGSMITH_API_KEY'] = ''\n",
    "os.environ['LANGSMITH_PROJECT'] = 'cohort-3-langgraph'\n",
    "\n",
    "# Pinecone Configuration\n",
    "os.environ[\"PINECONE_API_KEY\"] = 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9b9f0",
   "metadata": {},
   "source": [
    "## 3. Initialize LLM and Embeddings\n",
    "\n",
    "Create Azure OpenAI instances for chat and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341da8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM and Embeddings initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4.1\",\n",
    "    temperature=0.7,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "# Initialize Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "print(\"‚úì LLM and Embeddings initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a70b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: RAG System Evaluation\n",
    "\n",
    "We'll evaluate a RAG system using real chunks from Pinecone vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ff70c",
   "metadata": {},
   "source": [
    "## 4. Connect to Pinecone and Fetch Real Chunks\n",
    "\n",
    "Retrieve actual document chunks from the vector store for realistic testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10044ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rushi\\miniconda3\\envs\\aiproject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching real chunks from Pinecone...\n",
      "\n",
      "‚úì Query: What is the age eligibility for term insurance?\n",
      "  Retrieved 5 chunks\n",
      "  Sample: |Eligibility Conditions|Col2|\n",
      "|---|---|\n",
      "|Minimum Entry Age|16 years for Employer Employee schemes<br>(18 years if rider is chosen)<br>18 years for Non...\n",
      "\n",
      "‚úì Query: What are the benefits of group gratuity?\n",
      "  Retrieved 5 chunks\n",
      "  Sample: **HDFC Life Group Gratuity Product offers**\n",
      "\n",
      "\n",
      "- Range of Debt and Equity oriented funds to choose from\n",
      "\n",
      "- Flexibility of paying premiums\n",
      "\n",
      "- Control ov...\n",
      "\n",
      "‚úì Query: Who should I contact for grievances?\n",
      "  Retrieved 5 chunks\n",
      "  Sample: You may refer to the escalation matrix in case there is no response to a grievance within the\n",
      "prescribed timelines\n",
      "\n",
      "If you are still not satisfi ed wi...\n",
      "\n",
      "\n",
      "‚úì Successfully fetched chunks for 3 queries\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index('policy-agenticrag')\n",
    "\n",
    "# Create vector store\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "# Create retriever with score threshold\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.3},\n",
    ")\n",
    "\n",
    "# Define sample queries for testing\n",
    "sample_queries = [\n",
    "    \"What is the age eligibility for term insurance?\",\n",
    "    \"What are the benefits of group gratuity?\",\n",
    "    \"Who should I contact for grievances?\"\n",
    "]\n",
    "\n",
    "# Fetch and store chunks\n",
    "fetched_chunks = {}\n",
    "print(\"Fetching real chunks from Pinecone...\\n\")\n",
    "\n",
    "for query in sample_queries:\n",
    "    docs = retriever.invoke(query)\n",
    "    if docs:\n",
    "        fetched_chunks[query] = {\n",
    "            'contexts': [doc.page_content for doc in docs],\n",
    "            'metadata': [doc.metadata for doc in docs]\n",
    "        }\n",
    "        print(f\"‚úì Query: {query}\")\n",
    "        print(f\"  Retrieved {len(docs)} chunks\")\n",
    "        print(f\"  Sample: {docs[0].page_content[:150]}...\\n\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully fetched chunks for {len(fetched_chunks)} queries\")\n",
    "pinecone_available = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6a689",
   "metadata": {},
   "source": [
    "## 5. Create RAG Query Function\n",
    "\n",
    "Build a function that answers questions using the fetched chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbea58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the age eligibility for term insurance?\n",
      "\n",
      "Answer: **Answer:**\n",
      "\n",
      "The age eligibility for HDFC Life Group Term Insurance is as follows:\n",
      "\n",
      "- **Employer-Employee Schemes:**\n",
      "  - **Minimum Entry Age:** 16 years (18 years if rider is chosen)\n",
      "  - **Maximum Entry Age:** 79 years (64 years if rider is chosen)\n",
      "  - **Minimum Maturity Age:** 17 years (19 years if rider is chosen)\n",
      "\n",
      "- **Non Employer-Employee Schemes:**\n",
      "  - **Minimum Entry Age:** 18 years\n",
      "  - **Maximum Entry Age:** 79 years (64 years if rider is chosen)\n",
      "  - **Minimum Maturity Age:** 19 years\n",
      "\n",
      "- **Maximum Maturity Age (for all):** 80 years\n",
      "\n",
      "**Note:**  \n",
      "- Risk cover starts from the date of commencement of the policy for all lives, including minors. For minors, the policy vests on the Life Assured upon attaining age 18 years.\n",
      "\n",
      "Number of source documents: 5\n"
     ]
    }
   ],
   "source": [
    "def rag_query_with_real_chunks(question):\n",
    "    \"\"\"RAG system using real chunks fetched from vector store\"\"\"\n",
    "    \n",
    "    # Find matching chunks\n",
    "    relevant_contexts = []\n",
    "    \n",
    "    # Try exact match first\n",
    "    if question in fetched_chunks:\n",
    "        relevant_contexts = fetched_chunks[question]['contexts']\n",
    "    \n",
    "    # Generate answer using LLM\n",
    "    context = \"\\n\\n\".join(relevant_contexts)\n",
    "    \n",
    "    prompt = f\"\"\"You are an insurance policy assistant. Use the following context to answer the question.\n",
    "If you don't know the answer based on the context, say \"I don't have enough information to answer this.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        'result': response.content,\n",
    "        'source_documents': relevant_contexts\n",
    "    }\n",
    "\n",
    "# Test the RAG function\n",
    "test_query = \"What is the age eligibility for term insurance?\"\n",
    "test_result = rag_query_with_real_chunks(test_query)\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nAnswer: {test_result['result']}\")\n",
    "print(f\"\\nNumber of source documents: {len(test_result['source_documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6087e",
   "metadata": {},
   "source": [
    "## 6. Configure DeepEval with Azure OpenAI\n",
    "\n",
    "Create a custom DeepEval model wrapper to integrate with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7630afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DeepEval configured successfully with Azure OpenAI\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class AzureOpenAIModel(DeepEvalBaseLLM):\n",
    "    \"\"\"Custom DeepEval model wrapper for Azure OpenAI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = llm\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.model.invoke(prompt)\n",
    "        return response.content\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        response = await self.model.ainvoke(prompt)\n",
    "        return response.content\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return \"gpt-4.1\"\n",
    "\n",
    "# Initialize DeepEval model\n",
    "deepeval_model = AzureOpenAIModel()\n",
    "\n",
    "print(\"‚úì DeepEval configured successfully with Azure OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a422f",
   "metadata": {},
   "source": [
    "## 7. Create RAG Test Cases with Expected Outputs\n",
    "\n",
    "Generate test cases with expected answers for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4342dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 3 RAG test cases with expected outputs\n",
      "\n",
      "Sample Test Case:\n",
      "Question: What is the age eligibility for term insurance?\n",
      "Expected: The age eligibility for term insurance is minimum 18 years and maximum 65 years.\n",
      "Actual: **Answer:**\n",
      "\n",
      "The age eligibility for HDFC Life Group Term Insurance is as follows:\n",
      "\n",
      "- **Employer-Employee Schemes:**\n",
      "  - **Minimum Entry Age:** 16 years (18 years if rider is chosen)\n",
      "  - **Maximum Entry Age:** 79 years (64 years if rider is chosen)\n",
      "  - **Minimum Maturity Age:** 17 years (19 years if rider is chosen)\n",
      "\n",
      "- **Non Employer-Employee Schemes:**\n",
      "  - **Minimum Entry Age:** 18 years\n",
      "  - **Maximum Entry Age:** 79 years (64 years if rider is chosen)\n",
      "  - **Minimum Maturity Age:** 19 years\n",
      "\n",
      "- **Maximum Maturity Age for all:** 80 years\n",
      "\n",
      "**Note:**  \n",
      "- Risk cover starts from the date of commencement of policy for all lives, including minors.  \n",
      "- In case of a minor, the policy will vest on the Life Assured upon attaining age 18 years.\n"
     ]
    }
   ],
   "source": [
    "# Define test queries with expected answers\n",
    "rag_test_data = [\n",
    "    {\n",
    "        'query': 'What is the age eligibility for term insurance?',\n",
    "        'expected_output': 'The age eligibility for term insurance is minimum 18 years and maximum 65 years.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'What are the benefits of group gratuity?',\n",
    "        'expected_output': 'The benefits include tax-efficient retirement planning, lump sum payment upon retirement, professional fund management, flexible investment options, and death and disability benefits.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Who should I contact for grievances?',\n",
    "        'expected_output': 'For grievances, you can contact service@hdfclife.com or call toll-free 1860-267-9999. The Grievance Officer is Mr. Rajesh Kumar.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate RAG test cases\n",
    "rag_test_cases = []\n",
    "for test in rag_test_data:\n",
    "    result = rag_query_with_real_chunks(test['query'])\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=test['query'],\n",
    "        actual_output=result['result'],\n",
    "        expected_output=test['expected_output'],\n",
    "        retrieval_context=result['source_documents']\n",
    "    )\n",
    "    rag_test_cases.append(test_case)\n",
    "\n",
    "print(f\"‚úì Created {len(rag_test_cases)} RAG test cases with expected outputs\")\n",
    "print(\"\\nSample Test Case:\")\n",
    "print(f\"Question: {rag_test_cases[0].input}\")\n",
    "print(f\"Expected: {rag_test_cases[0].expected_output}\")\n",
    "print(f\"Actual: {rag_test_cases[0].actual_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a2bad",
   "metadata": {},
   "source": [
    "## 8. Run RAG Evaluations\n",
    "\n",
    "Execute comprehensive RAG evaluations with multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfa82c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\rushi\\miniconda3\\envs\\aiproject\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\rushi\\miniconda3\\envs\\aiproject\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running RAG Evaluations\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Answer Relevancy Score: 0.77\n",
      "   Reason: The score is 0.77 because the output included information about minimum maturity age and riders, which is irrelevant to age eligibility for entering term insurance. However, it still provided some relevant details about entry age, which is why the score isn't lower.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Faithfulness Score: 1.00\n",
      "   Reason: The score is 1.00 because there are no contradictions‚Äîthe actual output aligns perfectly with the retrieval context. Great job staying faithful to the source!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Contextual Relevancy Score: 0.18\n",
      "   Reason: The score is 0.18 because, despite the presence of highly relevant statements like 'Minimum Entry Age is 16 years for Employer Employee schemes (18 years if rider is chosen), 18 years for Non Employer Employee schemes.' and 'Maximum Entry Age is 79 years (64 years if rider is chosen).', the majority of the retrieval context consists of information unrelated to age eligibility, such as 'Policy Term is one year renewable' and 'Minimum Sum Assured is Rs. 10000.', which significantly lowers overall relevancy.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Correctness (vs Expected) Score: 0.20\n",
      "   Reason: The actual output provides detailed age eligibility criteria for different schemes and rider options, including minimum and maximum entry ages, as well as maturity ages. However, it does not align with the expected output, which simply states a minimum eligibility age of 18 and a maximum of 65. The actual output lists maximum entry ages up to 79, which contradicts the expected maximum of 65, and includes more nuanced details not requested. Therefore, the key information and figures do not match, and there are contradictions present.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualRelevancyMetric, GEval\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Running RAG Evaluations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Answer Relevancy - Are answers relevant to questions?\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=deepeval_model)\n",
    "answer_relevancy_metric.measure(rag_test_cases[0])\n",
    "print(f\"\\n1. Answer Relevancy Score: {answer_relevancy_metric.score:.2f}\")\n",
    "print(f\"   Reason: {answer_relevancy_metric.reason}\")\n",
    "\n",
    "# 2. Faithfulness - Are there hallucinations?\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7, model=deepeval_model)\n",
    "faithfulness_metric.measure(rag_test_cases[0])\n",
    "print(f\"\\n2. Faithfulness Score: {faithfulness_metric.score:.2f}\")\n",
    "print(f\"   Reason: {faithfulness_metric.reason}\")\n",
    "\n",
    "# 3. Contextual Relevancy - Is retrieved context relevant?\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7, model=deepeval_model)\n",
    "contextual_relevancy_metric.measure(rag_test_cases[0])\n",
    "print(f\"\\n3. Contextual Relevancy Score: {contextual_relevancy_metric.score:.2f}\")\n",
    "print(f\"   Reason: {contextual_relevancy_metric.reason}\")\n",
    "\n",
    "# 4. Correctness - Compare with expected output\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the actual output contains the key information from the expected output\",\n",
    "        \"Verify if the facts and figures match\",\n",
    "        \"Ensure there are no contradictions with the expected output\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    threshold=0.7,\n",
    "    model=deepeval_model\n",
    ")\n",
    "correctness_metric.measure(rag_test_cases[0])\n",
    "print(f\"\\n4. Correctness (vs Expected) Score: {correctness_metric.score:.2f}\")\n",
    "print(f\"   Reason: {correctness_metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ca222",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Agent Evaluation\n",
    "\n",
    "We'll evaluate an SQL agent's tool selection and execution accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a2447",
   "metadata": {},
   "source": [
    "## 9. Setup SQL Agent\n",
    "\n",
    "Create an agent with database tools for querying SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a712d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SQL Agent configured successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Define database tools\n",
    "def get_tables():\n",
    "    \"\"\"Return list of tables in the database.\"\"\"\n",
    "    conn = sqlite3.connect(\"sales_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return tables\n",
    "\n",
    "def get_schema(table_name: str):\n",
    "    \"\"\"Return schema of a given table.\"\"\"\n",
    "    conn = sqlite3.connect(\"sales_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "def execute_query(query: str):\n",
    "    \"\"\"Execute arbitrary SQL query and return results.\"\"\"\n",
    "    conn = sqlite3.connect(\"sales_data.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "# Create SQL agent\n",
    "sql_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[get_tables, get_schema, execute_query],\n",
    "    system_prompt=\"\"\"You are a sqlite database agent with tools to get tables, get schema and execute queries.\n",
    "    Understand column stats before generating queries.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úì SQL Agent configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c42335",
   "metadata": {},
   "source": [
    "## 10. Create Agent Test Cases with Expected Tools\n",
    "\n",
    "Generate test cases that specify which tools the agent should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ba2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Created 2 agent test cases successfully\n",
      "\n",
      "Sample Agent Test Case:\n",
      "Query: What tables exist in the database?\n",
      "Tools Called: ['get_tables']\n",
      "Expected Tools: ['get_tables']\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import ToolCall\n",
    "\n",
    "agent_test_data = [\n",
    "    {\n",
    "        'query': 'What tables exist in the database?',\n",
    "        'expected_tools': ['get_tables'],\n",
    "        'expected_output': 'Available tables in the database'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Show me the schema of the sales_data table',\n",
    "        'expected_tools': ['get_schema'],\n",
    "        'expected_output': 'Schema information for sales_data table'\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_test_cases = []\n",
    "for test in agent_test_data:\n",
    "    try:\n",
    "        result = sql_agent.invoke({'messages': [test['query']]})\n",
    "        \n",
    "        # Extract tools called and create ToolCall objects\n",
    "        tools_called = []\n",
    "        for message in result['messages']:\n",
    "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    tool_call_obj = ToolCall(\n",
    "                        name=tool_call['name'],\n",
    "                        input=tool_call.get('args', {})\n",
    "                    )\n",
    "                    if not any(tc.name == tool_call_obj.name for tc in tools_called):\n",
    "                        tools_called.append(tool_call_obj)\n",
    "        \n",
    "        # Convert expected tools to ToolCall objects\n",
    "        expected_tools = [ToolCall(name=tool, input={}) for tool in test['expected_tools']]\n",
    "        \n",
    "        agent_test_case = LLMTestCase(\n",
    "            input=test['query'],\n",
    "            actual_output=result['messages'][-1].content,\n",
    "            expected_output=test['expected_output'],\n",
    "            tools_called=tools_called,\n",
    "            expected_tools=expected_tools\n",
    "        )\n",
    "        agent_test_cases.append(agent_test_case)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Skipping test case: {test['query']}\")\n",
    "        print(f\"  Error: {str(e)[:100]}\")\n",
    "\n",
    "print(f\"\\n‚úì Created {len(agent_test_cases)} agent test cases successfully\")\n",
    "if agent_test_cases:\n",
    "    print(\"\\nSample Agent Test Case:\")\n",
    "    print(f\"Query: {agent_test_cases[0].input}\")\n",
    "    print(f\"Tools Called: {[tc.name for tc in agent_test_cases[0].tools_called]}\")\n",
    "    print(f\"Expected Tools: {[tc.name for tc in agent_test_cases[0].expected_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1d522",
   "metadata": {},
   "source": [
    "## 11. Run Agent Evaluations\n",
    "\n",
    "Evaluate tool selection accuracy and batch process all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf2bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\rushi\\miniconda3\\envs\\aiproject\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\rushi\\miniconda3\\envs\\aiproject\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running Agent Evaluations\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Tool Correctness Score: 1.00\n",
      "   Tools Called: ['get_tables']\n",
      "   Expected Tools: ['get_tables']\n",
      "   Match: ‚úì\n",
      "\n",
      "============================================================\n",
      "Batch Evaluation Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Tool Correctness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mTool Correctness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Tool Correctness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: [\n",
      "\t Tool Calling Reason: All expected tools ['get_tables'] were called (order not considered).\n",
      "\t Tool Selection Reason: No available tools were provided to assess tool selection criteria\n",
      "]\n",
      ", error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What tables exist in the database?\n",
      "  - actual output: There are currently no tables in the database. If you have any other requests or need to create tables, please let me know!\n",
      "  - expected output: Available tables in the database\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Tool Correctness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: [\n",
      "\t Tool Calling Reason: All expected tools ['get_schema'] were called (order not considered).\n",
      "\t Tool Selection Reason: No available tools were provided to assess tool selection criteria\n",
      "]\n",
      ", error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Show me the schema of the sales_data table\n",
      "  - actual output: It appears there was an issue retrieving the schema for the sales_data table. Would you like me to try again or assist you with something else related to the sales_data table?\n",
      "  - expected output: Schema information for sales_data table\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Tool Correctness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=30561;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>23s | token cost: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m0.\u001b[0m23s | token cost: \u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m2\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Evaluation Summary:\n",
      "Total Test Cases: 2\n",
      "Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Running Agent Evaluations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Tool Correctness - Single test case\n",
    "tool_correctness_metric = ToolCorrectnessMetric(threshold=0.7, model=deepeval_model)\n",
    "tool_correctness_metric.measure(agent_test_cases[0])\n",
    "print(f\"\\n1. Tool Correctness Score: {tool_correctness_metric.score:.2f}\")\n",
    "print(f\"   Tools Called: {[tc.name for tc in agent_test_cases[0].tools_called]}\")\n",
    "print(f\"   Expected Tools: {[tc.name for tc in agent_test_cases[0].expected_tools]}\")\n",
    "print(f\"   Match: {'‚úì' if tool_correctness_metric.score >= 0.7 else '‚úó'}\")\n",
    "\n",
    "# 2. Batch Evaluation - All test cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Batch Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = evaluate(\n",
    "    test_cases=agent_test_cases,\n",
    "    metrics=[ToolCorrectnessMetric(threshold=0.5, model=deepeval_model)]\n",
    ")\n",
    "\n",
    "print(f\"\\nAgent Evaluation Summary:\")\n",
    "print(f\"Total Test Cases: {len(agent_test_cases)}\")\n",
    "print(f\"Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa6202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comprehensive Evaluation Dashboard\n",
    "\n",
    "Summary of all RAG and Agent evaluations with insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef650cff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### RAG Evaluations:\n",
    "1. **Answer Relevancy** - Ensures answers match questions\n",
    "2. **Faithfulness** - Prevents hallucinations by grounding in context\n",
    "3. **Contextual Relevancy** - Validates retrieval quality\n",
    "4. **Correctness** - Compares with expected outputs\n",
    "\n",
    "### Agent Evaluations:\n",
    "1. **Tool Correctness** - Validates proper tool selection\n",
    "2. **Tool Arguments** - Ensures correct parameters\n",
    "3. **Expected Outputs** - Compares agent responses\n",
    "\n",
    "### Best Practices:\n",
    "- ‚úÖ Set appropriate thresholds (0.7 is common)\n",
    "- ‚úÖ Use batch evaluation for efficiency\n",
    "- ‚úÖ Create diverse test cases\n",
    "- ‚úÖ Monitor evaluation metrics over time\n",
    "- ‚úÖ Combine multiple metrics for comprehensive evaluation\n",
    "- ‚úÖ Use real data from production systems when possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
