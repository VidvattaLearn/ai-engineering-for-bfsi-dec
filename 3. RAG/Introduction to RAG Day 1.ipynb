{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# langsmith\n",
        "# embedding models and understand how similarity works\n",
        "# rag pipeline on a large html document using langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Observability\n",
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"api-version\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"endpoint\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"api-key\"\n",
        "\n",
        "os.environ['LANGSMITH_TRACING']='true'\n",
        "os.environ['LANGSMITH_ENDPOINT']='https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_API_KEY']='langsmith-key'\n",
        "os.environ['LANGSMITH_PROJECT']='project-name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = AzureChatOpenAI(\n",
        "  deployment_name = \"gpt-4.1\",\n",
        ")\n",
        "\n",
        "llm.invoke('hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-large\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector = embeddings.embed_query('ai agents are llms with superpower')\n",
        "\n",
        "len(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"The central bank increased interest rates to combat inflation.\",\n",
        "    \"A sports car with a V8 engine accelerates very quickly.\",\n",
        "    \"The Reserve Bank announced a hike in repo rate to curb inflation.\",\n",
        "    \"Electric cars are growing in popularity due to lower emissions.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
        "    \"\"\"Return cosine similarity between two 1-D numpy vectors.\"\"\"\n",
        "    # add tiny eps to denominator to avoid division by zero\n",
        "    denom = (norm(vec_a) * norm(vec_b)) + 1e-12\n",
        "    return float(np.dot(vec_a, vec_b) / denom)\n",
        "\n",
        "def euclidean_distance(vec_a, vec_b):\n",
        "  return  np.linalg.norm(vec_a - vec_b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectors = [np.array(embeddings.embed_query(text)) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vectors[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index in range(0, len(vectors)):\n",
        "  print(index, cosine_similarity(vectors[0], vectors[index]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index in range(0, len(vectors)):\n",
        "  print(index, euclidean_distance(vectors[0], vectors[index]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading Phase\n",
        "# Document Loaders\n",
        "# Chunking / Splitting\n",
        "# Embeddings\n",
        "# Vector DB\n",
        "\n",
        "# User Querying Phase\n",
        "# User Query\n",
        "# Embedding\n",
        "# Similarity Search\n",
        "# User Query + Similar Chunks\n",
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -Uq langchain_community unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import tempfile\n",
        "# from langchain.document_loaders import UnstructuredHTMLLoader\n",
        "amzn_def14a_doc = \"https://www.sec.gov/Archives/edgar/data/1018724/000110465925033442/tm252295-1_def14a.htm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download locally\n",
        "headers = {\n",
        "    \"User-Agent\": \"RushikeshAnalytics/2.0 (rushikesh@gmail.com)\"\n",
        "}\n",
        "\n",
        "response = requests.get(amzn_def14a_doc, headers=headers)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to fetch: {response.status_code}\")\n",
        "else:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\") as tmp_file:\n",
        "        tmp_file.write(response.content)\n",
        "        local_html_path = tmp_file.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "local_html_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = UnstructuredHTMLLoader(local_html_path)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(documents[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -qU langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "vector_store = Chroma.from_documents(\n",
        "    texts,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_finance3\"\n",
        ")\n",
        "\n",
        "# Data Loading Part : END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! ls chroma_finance3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever=vector_store.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = 'what is the ceo pay ratio'\n",
        "\n",
        "\n",
        "\n",
        "def get_rag_answers(user_query):\n",
        "  chunks = retriever.invoke(user_query) # get matching chunks from vector db\n",
        "  # embed user_query \n",
        "  # match embeddings with vector db chunks\n",
        "  # return top k chunks\n",
        "\n",
        "  \n",
        "  context = ' '.join([chunk.page_content for chunk in chunks]) # merge chunks into single string\n",
        "  prompt = f\"\"\"\n",
        "  You are a AI assitant, below is the user query and context, answer the user query based on the context only\n",
        "  user query : {user_query}\n",
        "  context: {context}\n",
        "  \"\"\"  \n",
        "  response = llm.invoke(prompt)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = ' '.join([chunk.page_content for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "You are a AI assitant, below is the user query and context, answer the user query based on the context only\n",
        "user query : {user_query}\n",
        "context: {context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = llm.invoke(prompt)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
